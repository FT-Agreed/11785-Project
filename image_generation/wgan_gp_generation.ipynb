{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# import torch\n",
    "# from torchvision.utils import save_image\n",
    "# import torch.nn as nn\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"HerbaceousVegetation\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip '/home/ubuntu/efs/data/archive(1).zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_data_dir = \"/home/ubuntu/efs/data/archive/RBG/train/HerbaceousVegetation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_images = tf.keras.utils.image_dataset_from_directory(\n",
    "    new_data_dir, label_mode=None, image_size=(64, 64), batch_size=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_batch = next(iter(train_images))\n",
    "random_index = np.random.choice(image_batch.shape[0])\n",
    "random_image = image_batch[random_index].numpy().astype(\"int32\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(random_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalize the images to [-1, 1] which is the range of the tanh activation\n",
    "train_images = train_images.map(lambda x: (x - 127.5) / 127.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# latent dimension of the random noise\n",
    "LATENT_DIM = 128\n",
    "# weight initializer for G per DCGAN paper \n",
    "WEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02) \n",
    "# number of channels, 1 for gray scale and 3 for color images\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    # create a Keras Sequential model \n",
    "    model = Sequential(name=\"generator\")\n",
    "\n",
    "    # prepare for reshape: FC => BN => RN layers, note: input shape defined in the 1st Dense layer  \n",
    "    model.add(layers.Dense(8 * 8 * 512, input_dim=LATENT_DIM))\n",
    "    # model.add(layers.BatchNormalization()) \n",
    "    model.add(layers.ReLU())\n",
    "    # layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "    # 1D => 3D: reshape the output of the previous layer \n",
    "    model.add(layers.Reshape((8, 8, 512)))\n",
    "\n",
    "    # upsample to 16x16: apply a transposed CONV => BN => RELU\n",
    "    model.add(layers.Conv2DTranspose(256, (4, 4), \n",
    "                                     strides=(2, 2),\n",
    "                                     padding=\"same\", \n",
    "                                     use_bias=False, \n",
    "                                     kernel_initializer=WEIGHT_INIT))\n",
    "    model.add(layers.BatchNormalization()) \n",
    "    model.add((layers.ReLU()))\n",
    "\n",
    "    # upsample to 32x32: apply a transposed CONV => BN => RELU\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), \n",
    "                                     strides=(2, 2),\n",
    "                                     padding=\"same\", \n",
    "                                     use_bias=False, \n",
    "                                     kernel_initializer=WEIGHT_INIT))\n",
    "    model.add(layers.BatchNormalization()) \n",
    "    model.add((layers.ReLU()))\n",
    "\n",
    "    # upsample to 64x64: apply a transposed CONV => BN => RELU\n",
    "    model.add(layers.Conv2DTranspose(4, (4, 4), \n",
    "                                     strides=(2, 2),\n",
    "                                     padding=\"same\", \n",
    "                                     use_bias=False, \n",
    "                                     kernel_initializer=WEIGHT_INIT))\n",
    "    model.add(layers.BatchNormalization()) \n",
    "    model.add((layers.ReLU()))\n",
    "\n",
    "    # final layer: Conv2D with tanh activation\n",
    "    model.add(layers.Conv2D(CHANNELS, (4, 4), padding=\"same\", activation=\"tanh\"))\n",
    "\n",
    "    # return the generator model\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the generator model\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32768)             4227072   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 32768)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 16, 16, 256)      2097152   \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 16, 16, 256)      1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 128)      524288    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 4)        8192      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64, 64, 4)        16        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 64, 64, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 64, 3)         195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,858,451\n",
      "Trainable params: 6,857,675\n",
      "Non-trainable params: 776\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UPDATE for WGAN-GP: no more weight clipping\n",
    "# class WeightClip(tf.keras.constraints.Constraint):\n",
    "#     def __init__(self, clip_value):\n",
    "#         self.clip_value = clip_value\n",
    "    \n",
    "#     def __call__(self, weights):\n",
    "#         return tf.clip_by_value(weights, -self.clip_value, self.clip_value)\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         return {'clip_value': self.clip_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_critic(height, width, depth, alpha=0.2):\n",
    "    # Weight clipping to [-1, 1] to enfoce 1-Lipschitz constraint\n",
    "    # constraint = WeightClip(0.01) # UPDATE for WGAN-GP: remove weight clipping\n",
    "\n",
    "    # create a Keras Sequential model\n",
    "    model = Sequential(name=\"critic\")\n",
    "    input_shape = (height, width, depth)\n",
    "\n",
    "    # 1. first set of CONV => BN => leaky ReLU layers\n",
    "    model.add(layers.Conv2D(64, (4, 4), \n",
    "                            padding=\"same\", \n",
    "                            strides=(2, 2), \n",
    "                            # kernel_constraint = constraint, # UPDATE for WGAN-GP: remove weight clipping\n",
    "                            input_shape=input_shape))\n",
    "    # model.add(layers.BatchNormalization()) # UPDATE for WGAN-GP: remove batchnorm\n",
    "    model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "    # 2. second set of CONV => BN => leacy ReLU layers\n",
    "    model.add(layers.Conv2D(128, (4, 4), \n",
    "                            padding=\"same\", \n",
    "                            strides=(2, 2))) \n",
    "                            # kernel_constraint = constraint,)) # UPDATE for WGAN-GP: remove weight clipping\n",
    "    # model.add(layers.BatchNormalization()) # UPDATE for WGAN-GP: remove batchnorm\n",
    "    model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "    # 3. third set of CONV => BN => leacy ReLU layers\n",
    "    model.add(layers.Conv2D(128, (4, 4), \n",
    "                            padding=\"same\", \n",
    "                            strides=(2, 2))) \n",
    "                            # kernel_constraint = constraint,)) # UPDATE for WGAN-GP: remove weight clipping\n",
    "    # model.add(layers.BatchNormalization()) # UPDATE for WGAN-GP: remove batchnorm\n",
    "    model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "    # flatten and apply dropout\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.3)) \n",
    "\n",
    "    # linear activation in the last layer \n",
    "    # Note: Keras `Dense` layer by default is already a `linear` activation\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # return the critic model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the critic model\n",
    "critic = build_critic(64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"critic\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        3136      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       131200    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 8193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def build_discriminator(height, width, depth, alpha=0.2):\n",
    "#     # create a Keras Sequential model\n",
    "#     model = Sequential(name='discriminator')\n",
    "#     input_shape = (height, width, depth)\n",
    "\n",
    "#     # 1. first set of CONV => BN => leaky ReLU layers\n",
    "#     model.add(layers.Conv2D(64, (4, 4), padding=\"same\", strides=(2, 2),\n",
    "#         input_shape=input_shape))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "#     # 2. second set of CONV => BN => leacy ReLU layers\n",
    "#     model.add(layers.Conv2D(128, (4, 4), padding=\"same\", strides=(2, 2)))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "#     # 3. third set of CONV => BN => leacy ReLU layers\n",
    "#     model.add(layers.Conv2D(128, (4, 4), padding=\"same\", strides=(2, 2)))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU(alpha=alpha))\n",
    "\n",
    "#     # flatten and apply dropout\n",
    "#     model.add(layers.Flatten())\n",
    "#     model.add(layers.Dropout(0.3))\n",
    "\n",
    "#     # sigmoid in the last layer outputting a single value for binary classification\n",
    "#     model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "#     # return the discriminator model\n",
    "#     return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # build the discriminator model\n",
    "# discriminator = build_discriminator(64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WGAN_GP(keras.Model):\n",
    "    def __init__(self, \n",
    "                 critic, \n",
    "                 generator, \n",
    "                 latent_dim, \n",
    "                 critic_extra_steps, \n",
    "                 gp_weight=10.0): # UPDATE for WGAN-GP: gradient penalty weight\n",
    "        super().__init__()\n",
    "        self.critic = critic\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.c_extra_steps = critic_extra_steps\n",
    "        self.gp_weight = gp_weight # WGAN-GP\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN_GP, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    # UPDATE for WGAN-GP: use gradient penalty instead of weight clipping\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        Gradient penalty is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        # 1. Create the interpolated image\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 2. Get the Critic's output for the interpolated image\n",
    "            pred = self.critic(interpolated, training=True)\n",
    "\n",
    "        # 3. Calculate the gradients w.r.t to the interpolated image\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 4. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        # 5. Calculate gradient penalty\n",
    "        gradient_penalty = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gradient_penalty\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Train the critic more often than the generator by 5 times (self.c_extra_steps) \n",
    "        for i in range(self.c_extra_steps):\n",
    "            # Step 1. Train the critic with both real images and fake images\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred_real = self.critic(real_images, training=True)\n",
    "                fake_images = self.generator(noise, training=True) \n",
    "                pred_fake = self.critic(fake_images, training=True)\n",
    "                # UPDATE for WGAN-GP: Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # UPDATE for WGAN-GP: Add gradient penalty to the original critic loss \n",
    "                d_loss = self.d_loss_fn(pred_real, pred_fake) + gp * self.gp_weight \n",
    "            # Compute critic gradients\n",
    "            grads = tape.gradient(d_loss, self.critic.trainable_variables)\n",
    "            # Update critic weights\n",
    "            self.d_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "\n",
    "        # Step 2. Train the generator (do not update weights of the critic)\n",
    "        misleading_labels = tf.ones((batch_size, 1)) # G wants D to think the fake images are real (label as 1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(noise, training=True)\n",
    "            pred_fake = self.critic(fake_images, training=True)\n",
    "            g_loss = self.g_loss_fn(pred_fake)\n",
    "        # Compute generator gradients\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update generator wieghts\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
    "\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "\n",
    "        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=256, latent_dim=100):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        #self.right_path = 'Forest' + str(p)\n",
    "\n",
    "        # Create random noise seed for visualization during traing\n",
    "        self.seed = tf.random.normal([256, latent_dim])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        generated_images = self.model.generator(self.seed)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "        generated_images= generated_images.numpy()\n",
    "        \n",
    "\n",
    "        if epoch ==299:\n",
    "            #fig = plt.figure(figsize=(64, 64))\n",
    "            px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "            fig = plt.figure(figsize=(64*px, 64*px))\n",
    "            #plt.subplots(figsize=(600*px, 200*px))\n",
    "            for i in range(self.num_img):\n",
    "                #plt.subplot(4, 4, i+1)\n",
    "                img = keras.utils.array_to_img(generated_images[i]) \n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.savefig('HerbaceousVegetation/HerbaceousVegetation{:03d}.png'.format(i))\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wgan_gp = WGAN_GP(critic=critic, \n",
    "              generator=generator, \n",
    "              latent_dim=LATENT_DIM,\n",
    "              critic_extra_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wasserstein loss for the critic\n",
    "def d_wasserstein_loss(pred_real, pred_fake):\n",
    "    real_loss = tf.reduce_mean(pred_real)\n",
    "    fake_loss = tf.reduce_mean(pred_fake)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "# Wasserstein loss for the generator\n",
    "def g_wasserstein_loss(pred_fake):\n",
    "    return -tf.reduce_mean(pred_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 0.0002 # WGAN-GP paper recommends lr of 0.0002\n",
    "d_optimizer = keras.optimizers.Adam(learning_rate=LR, beta_1=0.5, beta_2=0.9) # UPDATE for WGAN-GP: use Adam instead of RMSProp\n",
    "g_optimizer = keras.optimizers.Adam(learning_rate=LR, beta_1=0.5, beta_2=0.9) # UPDATE for WGAN-GP: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wgan_gp.compile(\n",
    "    d_optimizer=d_optimizer,\n",
    "    g_optimizer=g_optimizer,  \n",
    "    d_loss_fn = d_wasserstein_loss,\n",
    "    g_loss_fn = g_wasserstein_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# D_LR = 0.0001 # UPDATED: discriminator learning rate\n",
    "# G_LR = 0.0003 # UPDATED: generator learning rate\n",
    "\n",
    "# dcgan.compile(\n",
    "#     d_optimizer=keras.optimizers.Adam(learning_rate=D_LR, beta_1 = 0.5),\n",
    "#     g_optimizer=keras.optimizers.Adam(learning_rate=G_LR, beta_1 = 0.5),  \n",
    "#     loss_fn=keras.losses.BinaryCrossentropy(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "150/150 [==============================] - 12s 39ms/step - d_loss: -42.1138 - g_loss: 53.7832\n",
      "Epoch 2/300\n",
      "150/150 [==============================] - 6s 38ms/step - d_loss: -13.7716 - g_loss: 54.0455\n",
      "Epoch 3/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -12.4052 - g_loss: 48.5514\n",
      "Epoch 4/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -12.4011 - g_loss: 60.0093\n",
      "Epoch 5/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -12.2081 - g_loss: 69.0852\n",
      "Epoch 6/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -11.0941 - g_loss: 69.2824\n",
      "Epoch 7/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -10.0534 - g_loss: 76.6870\n",
      "Epoch 8/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -10.0909 - g_loss: 102.2469\n",
      "Epoch 9/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -9.9844 - g_loss: 88.3910\n",
      "Epoch 10/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -9.4195 - g_loss: 78.7014\n",
      "Epoch 11/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -7.9794 - g_loss: 102.6896\n",
      "Epoch 12/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -8.2699 - g_loss: 94.9060\n",
      "Epoch 13/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -8.6644 - g_loss: 96.8468\n",
      "Epoch 14/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -7.1284 - g_loss: 89.1828\n",
      "Epoch 15/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -6.5384 - g_loss: 89.1606\n",
      "Epoch 16/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -7.1155 - g_loss: 38.6307\n",
      "Epoch 17/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -6.6497 - g_loss: 84.6199\n",
      "Epoch 18/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -6.7045 - g_loss: 82.3488\n",
      "Epoch 19/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -5.2112 - g_loss: 41.8172\n",
      "Epoch 20/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -5.9812 - g_loss: -60.1546\n",
      "Epoch 21/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -4.6103 - g_loss: -56.2507\n",
      "Epoch 22/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -4.6761 - g_loss: -40.9072\n",
      "Epoch 23/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -4.9502 - g_loss: -44.8592\n",
      "Epoch 24/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -5.7136 - g_loss: -124.1507\n",
      "Epoch 25/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -6.3313 - g_loss: -132.3786\n",
      "Epoch 26/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -5.5548 - g_loss: -131.1689\n",
      "Epoch 27/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.5170 - g_loss: -123.7506\n",
      "Epoch 28/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.2986 - g_loss: -99.9094\n",
      "Epoch 29/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -5.0831 - g_loss: -74.2877\n",
      "Epoch 30/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -4.3724 - g_loss: -84.7316\n",
      "Epoch 31/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.1155 - g_loss: -33.2043\n",
      "Epoch 32/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -4.4119 - g_loss: -57.4711\n",
      "Epoch 33/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.7141 - g_loss: -53.6201\n",
      "Epoch 34/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.8026 - g_loss: -22.5932\n",
      "Epoch 35/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -4.2280 - g_loss: 14.4426\n",
      "Epoch 36/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -4.0962 - g_loss: 56.6366\n",
      "Epoch 37/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -4.7687 - g_loss: 74.9488\n",
      "Epoch 38/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.1566 - g_loss: 36.0242\n",
      "Epoch 39/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.1597 - g_loss: 32.0307\n",
      "Epoch 40/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.2579 - g_loss: 53.3058\n",
      "Epoch 41/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.2456 - g_loss: 64.1041\n",
      "Epoch 42/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.5312 - g_loss: 100.8453\n",
      "Epoch 43/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.3182 - g_loss: -8.7104\n",
      "Epoch 44/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.1781 - g_loss: -67.9309\n",
      "Epoch 45/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.7575 - g_loss: -71.9691\n",
      "Epoch 46/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.7608 - g_loss: -51.3183\n",
      "Epoch 47/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.1247 - g_loss: -37.4045\n",
      "Epoch 48/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.0545 - g_loss: -46.0229\n",
      "Epoch 49/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.0452 - g_loss: -27.3575\n",
      "Epoch 50/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.9398 - g_loss: -70.2277\n",
      "Epoch 51/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.2608 - g_loss: -48.4237\n",
      "Epoch 52/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.1725 - g_loss: -112.2558\n",
      "Epoch 53/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -1.7607 - g_loss: -110.1304\n",
      "Epoch 54/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.1596 - g_loss: -71.8467\n",
      "Epoch 55/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.3615 - g_loss: -77.8017\n",
      "Epoch 56/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.4065 - g_loss: -72.5900\n",
      "Epoch 57/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -4.4753 - g_loss: -118.4366\n",
      "Epoch 58/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.1015 - g_loss: -144.6693\n",
      "Epoch 59/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.7330 - g_loss: -121.8782\n",
      "Epoch 60/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -4.4380 - g_loss: -109.4834\n",
      "Epoch 61/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.6614 - g_loss: -119.3571\n",
      "Epoch 62/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.8401 - g_loss: -162.0448\n",
      "Epoch 63/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.5189 - g_loss: -146.2166\n",
      "Epoch 64/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.9420 - g_loss: -135.1963\n",
      "Epoch 65/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.3027 - g_loss: -81.0730\n",
      "Epoch 66/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.9331 - g_loss: -133.4840\n",
      "Epoch 67/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.5925 - g_loss: -135.0478\n",
      "Epoch 68/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.2359 - g_loss: -108.9784\n",
      "Epoch 69/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.5891 - g_loss: -108.0858\n",
      "Epoch 70/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.4947 - g_loss: -93.4848\n",
      "Epoch 71/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.1747 - g_loss: -132.8201\n",
      "Epoch 72/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.5642 - g_loss: -178.4073\n",
      "Epoch 73/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.9283 - g_loss: -145.0297\n",
      "Epoch 74/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.3555 - g_loss: -108.4364\n",
      "Epoch 75/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.6684 - g_loss: -69.6207\n",
      "Epoch 76/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.3835 - g_loss: -97.1115\n",
      "Epoch 77/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.0563 - g_loss: -109.2303\n",
      "Epoch 78/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -5.9034 - g_loss: -159.1332\n",
      "Epoch 79/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 0.1647 - g_loss: -142.3706\n",
      "Epoch 80/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.2660 - g_loss: -98.2231\n",
      "Epoch 81/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.2725 - g_loss: -130.6966\n",
      "Epoch 82/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.8707 - g_loss: -157.5592\n",
      "Epoch 83/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -2.7073 - g_loss: -183.9024\n",
      "Epoch 84/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.5976 - g_loss: -168.5448\n",
      "Epoch 85/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.8130 - g_loss: -140.0456\n",
      "Epoch 86/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -1.8047 - g_loss: -167.6985\n",
      "Epoch 87/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.0716 - g_loss: -119.6986\n",
      "Epoch 88/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.6892 - g_loss: -94.5812\n",
      "Epoch 89/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.0249 - g_loss: -109.8905\n",
      "Epoch 90/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.4213 - g_loss: -109.3117\n",
      "Epoch 91/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.9792 - g_loss: -102.2568\n",
      "Epoch 92/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -2.0053 - g_loss: -106.4142\n",
      "Epoch 93/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.3668 - g_loss: -77.3845\n",
      "Epoch 94/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -4.6396 - g_loss: -125.3280\n",
      "Epoch 95/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.5438 - g_loss: -127.2172\n",
      "Epoch 96/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.3042 - g_loss: -96.4014\n",
      "Epoch 97/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.8827 - g_loss: -89.6000\n",
      "Epoch 98/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.1000 - g_loss: -88.7112\n",
      "Epoch 99/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -4.5215 - g_loss: -111.6915\n",
      "Epoch 100/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.9420 - g_loss: -157.5484\n",
      "Epoch 101/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -2.5969 - g_loss: -152.3432\n",
      "Epoch 102/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.5480 - g_loss: -157.2337\n",
      "Epoch 103/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.2904 - g_loss: -184.9817\n",
      "Epoch 104/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.5519 - g_loss: -121.0015\n",
      "Epoch 105/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.1900 - g_loss: -181.1750\n",
      "Epoch 106/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.1221 - g_loss: -141.8918\n",
      "Epoch 107/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.8152 - g_loss: -78.4258\n",
      "Epoch 108/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.2968 - g_loss: -158.4271\n",
      "Epoch 109/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.1014 - g_loss: -251.2894\n",
      "Epoch 110/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.9725 - g_loss: -91.1694\n",
      "Epoch 111/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.0207 - g_loss: -66.6626\n",
      "Epoch 112/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.4965 - g_loss: -144.4933\n",
      "Epoch 113/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -4.4497 - g_loss: -194.1050\n",
      "Epoch 114/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.0936 - g_loss: -180.0193\n",
      "Epoch 115/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.4161 - g_loss: -132.8354\n",
      "Epoch 116/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -1.1605 - g_loss: -192.1393\n",
      "Epoch 117/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.6490 - g_loss: -265.4281\n",
      "Epoch 118/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -2.6975 - g_loss: -358.9138\n",
      "Epoch 119/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.5986 - g_loss: -339.8684\n",
      "Epoch 120/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.0527 - g_loss: -178.9270\n",
      "Epoch 121/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.8207 - g_loss: -128.4137\n",
      "Epoch 122/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -1.3989 - g_loss: -75.2026\n",
      "Epoch 123/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.9625 - g_loss: 31.7344\n",
      "Epoch 124/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.6768 - g_loss: 144.9835\n",
      "Epoch 125/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.9322 - g_loss: 175.8367\n",
      "Epoch 126/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -1.7441 - g_loss: 184.6828\n",
      "Epoch 127/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.4547 - g_loss: 194.9744\n",
      "Epoch 128/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.6728 - g_loss: 98.1175\n",
      "Epoch 129/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.1259 - g_loss: 77.5658\n",
      "Epoch 130/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.8925 - g_loss: 135.3484\n",
      "Epoch 131/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.5982 - g_loss: 250.9573\n",
      "Epoch 132/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.7580 - g_loss: 245.5235\n",
      "Epoch 133/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.3732 - g_loss: 213.9211\n",
      "Epoch 134/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.1804 - g_loss: 206.8383\n",
      "Epoch 135/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.0857 - g_loss: 290.5312\n",
      "Epoch 136/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.6357 - g_loss: 157.6290\n",
      "Epoch 137/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.9068 - g_loss: 212.4431\n",
      "Epoch 138/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.7560 - g_loss: 256.0912\n",
      "Epoch 139/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.6850 - g_loss: 229.5045\n",
      "Epoch 140/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.4790 - g_loss: 194.7708\n",
      "Epoch 141/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -0.6463 - g_loss: 161.2437\n",
      "Epoch 142/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.1931 - g_loss: 154.8316\n",
      "Epoch 143/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.7695 - g_loss: 241.1659\n",
      "Epoch 144/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -2.0361 - g_loss: 170.5982\n",
      "Epoch 145/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.6030 - g_loss: 173.3485\n",
      "Epoch 146/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.3672 - g_loss: 122.9188\n",
      "Epoch 147/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.8175 - g_loss: 131.9995\n",
      "Epoch 148/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.9474 - g_loss: 235.3013\n",
      "Epoch 149/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.5440 - g_loss: 266.2439\n",
      "Epoch 150/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.8546 - g_loss: 178.7110\n",
      "Epoch 151/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.4093 - g_loss: 227.5807\n",
      "Epoch 152/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.7329 - g_loss: 245.9125\n",
      "Epoch 153/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.5328 - g_loss: 180.5220\n",
      "Epoch 154/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.0068 - g_loss: 167.5928\n",
      "Epoch 155/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.7959 - g_loss: 180.6724\n",
      "Epoch 156/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.4193 - g_loss: 218.8161\n",
      "Epoch 157/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -5.3146 - g_loss: 275.0398\n",
      "Epoch 158/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.1552 - g_loss: 384.6602\n",
      "Epoch 159/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.8417 - g_loss: 357.5155\n",
      "Epoch 160/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -0.5915 - g_loss: 336.9575\n",
      "Epoch 161/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.7541 - g_loss: 220.5937\n",
      "Epoch 162/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.7930 - g_loss: 302.5816\n",
      "Epoch 163/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -5.7751 - g_loss: 371.7134\n",
      "Epoch 164/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.4336 - g_loss: 322.5031\n",
      "Epoch 165/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.1231 - g_loss: 395.1431\n",
      "Epoch 166/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.5683 - g_loss: 403.1526\n",
      "Epoch 167/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.3651 - g_loss: 310.0963\n",
      "Epoch 168/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -5.4813 - g_loss: 450.3246\n",
      "Epoch 169/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.9478 - g_loss: 413.2039\n",
      "Epoch 170/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.7827 - g_loss: 350.1624\n",
      "Epoch 171/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -4.3197 - g_loss: 493.3627\n",
      "Epoch 172/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 0.4545 - g_loss: 421.4991\n",
      "Epoch 173/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: 2.1382 - g_loss: 266.7351\n",
      "Epoch 174/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 0.6914 - g_loss: 495.3498\n",
      "Epoch 175/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.0408 - g_loss: 618.8120\n",
      "Epoch 176/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 2.6776 - g_loss: 353.4736\n",
      "Epoch 177/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -0.5550 - g_loss: 317.8569\n",
      "Epoch 178/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.8374 - g_loss: 464.2124\n",
      "Epoch 179/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.9429 - g_loss: 388.9906\n",
      "Epoch 180/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -1.1540 - g_loss: 225.6096\n",
      "Epoch 181/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.2682 - g_loss: 214.7355\n",
      "Epoch 182/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -2.0025 - g_loss: 104.7394\n",
      "Epoch 183/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.4648 - g_loss: 190.5002\n",
      "Epoch 184/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: 2.3456 - g_loss: 289.5119\n",
      "Epoch 185/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 2.2574 - g_loss: 303.5233\n",
      "Epoch 186/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.6029 - g_loss: 231.4989\n",
      "Epoch 187/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -0.7226 - g_loss: 346.4262\n",
      "Epoch 188/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.0031 - g_loss: 167.8374\n",
      "Epoch 189/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -6.4588 - g_loss: 217.0991\n",
      "Epoch 190/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: 0.1050 - g_loss: 325.9043\n",
      "Epoch 191/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -2.8429 - g_loss: 302.1028\n",
      "Epoch 192/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 0.8254 - g_loss: 204.1045\n",
      "Epoch 193/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.6756 - g_loss: 270.4008\n",
      "Epoch 194/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: 1.6300 - g_loss: 337.2467\n",
      "Epoch 195/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.1791 - g_loss: 392.1226\n",
      "Epoch 196/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.2081 - g_loss: 274.1183\n",
      "Epoch 197/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 0.3253 - g_loss: 210.9483\n",
      "Epoch 198/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.5194 - g_loss: 205.4921\n",
      "Epoch 199/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.9956 - g_loss: 174.6844\n",
      "Epoch 200/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: 0.6405 - g_loss: 364.2486\n",
      "Epoch 201/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.8429 - g_loss: 347.3828\n",
      "Epoch 202/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.2313 - g_loss: 354.4468\n",
      "Epoch 203/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.0218 - g_loss: 360.2202\n",
      "Epoch 204/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.7638 - g_loss: 258.1183\n",
      "Epoch 205/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.2506 - g_loss: 235.0031\n",
      "Epoch 206/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -0.9418 - g_loss: 391.3177\n",
      "Epoch 207/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.8842 - g_loss: 256.9006\n",
      "Epoch 208/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.5783 - g_loss: 231.9536\n",
      "Epoch 209/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.7942 - g_loss: 205.1741\n",
      "Epoch 210/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.7519 - g_loss: 181.3442\n",
      "Epoch 211/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.3697 - g_loss: 318.4706\n",
      "Epoch 212/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.3490 - g_loss: 357.6987\n",
      "Epoch 213/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -4.1280 - g_loss: 419.2606\n",
      "Epoch 214/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.2926 - g_loss: 380.0351\n",
      "Epoch 215/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: 0.9801 - g_loss: 255.1414\n",
      "Epoch 216/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.6504 - g_loss: 263.6243\n",
      "Epoch 217/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.7542 - g_loss: 312.7050\n",
      "Epoch 218/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -4.3190 - g_loss: 333.5446\n",
      "Epoch 219/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -7.3924 - g_loss: 217.6102\n",
      "Epoch 220/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.4332 - g_loss: 195.9966\n",
      "Epoch 221/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -0.5601 - g_loss: 192.0881\n",
      "Epoch 222/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.9179 - g_loss: 149.4232\n",
      "Epoch 223/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: 0.1355 - g_loss: 157.1459\n",
      "Epoch 224/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: 1.1233 - g_loss: 276.8015\n",
      "Epoch 225/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.4831 - g_loss: 259.9111\n",
      "Epoch 226/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.6881 - g_loss: 243.0686\n",
      "Epoch 227/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.5261 - g_loss: 313.4843\n",
      "Epoch 228/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -0.3922 - g_loss: 227.0250\n",
      "Epoch 229/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.7160 - g_loss: 183.8149\n",
      "Epoch 230/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -5.4871 - g_loss: 353.8229\n",
      "Epoch 231/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.6195 - g_loss: 499.9688\n",
      "Epoch 232/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -2.1469 - g_loss: 438.3039\n",
      "Epoch 233/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -3.3899 - g_loss: 460.1506\n",
      "Epoch 234/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.5473 - g_loss: 237.9041\n",
      "Epoch 235/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: 0.1317 - g_loss: 333.1917\n",
      "Epoch 236/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.2849 - g_loss: 358.4229\n",
      "Epoch 237/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 0.7188 - g_loss: 329.9264\n",
      "Epoch 238/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.3066 - g_loss: 496.9567\n",
      "Epoch 239/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -3.4321 - g_loss: 221.9358\n",
      "Epoch 240/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.0716 - g_loss: 218.5144\n",
      "Epoch 241/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -0.8609 - g_loss: 234.0287\n",
      "Epoch 242/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.4855 - g_loss: 180.3064\n",
      "Epoch 243/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -3.0845 - g_loss: 235.1298\n",
      "Epoch 244/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -6.5515 - g_loss: 351.6983\n",
      "Epoch 245/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.7481 - g_loss: 320.2751\n",
      "Epoch 246/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.3378 - g_loss: 284.0368\n",
      "Epoch 247/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -4.8021 - g_loss: 193.5259\n",
      "Epoch 248/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -5.2076 - g_loss: 245.4822\n",
      "Epoch 249/300\n",
      "150/150 [==============================] - 6s 37ms/step - d_loss: -1.3030 - g_loss: 281.4404\n",
      "Epoch 250/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: 1.5290 - g_loss: 378.0061\n",
      "Epoch 251/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.6535 - g_loss: 279.3347\n",
      "Epoch 252/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.4090 - g_loss: 301.8685\n",
      "Epoch 253/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.5134 - g_loss: 193.5419\n",
      "Epoch 254/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.7727 - g_loss: 289.3120\n",
      "Epoch 255/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -2.5438 - g_loss: 382.2063\n",
      "Epoch 256/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -0.5661 - g_loss: 268.4969\n",
      "Epoch 257/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.5260 - g_loss: 231.5249\n",
      "Epoch 258/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.5990 - g_loss: 93.8397\n",
      "Epoch 259/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -2.0148 - g_loss: 131.7520\n",
      "Epoch 260/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -0.5103 - g_loss: 121.2699\n",
      "Epoch 261/300\n",
      "150/150 [==============================] - 5s 37ms/step - d_loss: -1.8721 - g_loss: 61.1879\n",
      "Epoch 262/300\n",
      "150/150 [==============================] - 5s 36ms/step - d_loss: -1.2650 - g_loss: -8.9625\n",
      "Epoch 263/300\n",
      " 21/150 [===>..........................] - ETA: 4s - d_loss: -1.7570 - g_loss: 18.8833"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 300 # number of epochs\n",
    "wgan_gp.fit(train_images, epochs=NUM_EPOCHS, callbacks=[GANMonitor(num_img=256, latent_dim=LATENT_DIM)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
